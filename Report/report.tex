\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[british]{babel}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
%% To use hyperlinks
\usepackage{hyperref}
%% To use colors
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

%% To use maths
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
\usepackage{titlesec}

%% Command to type a chi character
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} 
%% Command to simulate a tab character
\newcommand\tab[1][1cm]{\hspace*{#1}}

%% Set sub of subsections
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}



\title{\textbf{Aprendizagem Autom\'{a}tica} \\
	\large Assignment 2 - Clustering}

\author{Andrea Mascaretti N52222\and Daniel Pimenta N45404}

\begin{document}
	\maketitle
	\begin{abstract}
		TODO
	\end{abstract}


	\section{Introduction}
	\section{Clustering Algorithms}
	\subsection{K-Means Algorithm}
	In our exposition of the K-Means Algorithm we rely heavily on \cite{hastie01statisticallearning}, \cite{MR2372475} and \cite{zaki2014dataminingbook}.
	Suppose that we have a set of $n$ observations, indexed by $I = \{1, ..., n\}$, of some quantitative
	variables
	$$ C = \{ \boldsymbol{x}_{1}, \boldsymbol{x}_{2}, ..., \boldsymbol{x}_{n} \} $$
	where
	
	$$ \boldsymbol{x}_{i} \in \mathbb{R}^{p},\;\forall i\in I$$
	
	
	We also have a set of labels (at this stage we will consider their
	number as given and such that $card(\mathcal{L})=k,\:k\in\left[1,n\right]\cap\mathbb{N}$)
	
	$$
	\mathcal{L}=\left\{ l_{1},\ldots,l_{k}\right\} ,\;K:=\left\{ 1,\ldots,k\right\} 
	$$
	Our goal is to construct a (\textit{measurable}) function to associate each
	and every element of $C$ to one label in $\mathcal{L}$. It is easy
	to see that it makes no difference to work with $\mathcal{L}$ or
	$K$ as we can always find a bijection between the two sets.
	
	We have reasons to believe that the elements belonging to one particular
	group are somewhat more similar to each other than they are to members
	of other groups and that one element can belong to one group and one
	group only. We also hypothesise that there are no empty groups. Translating
	this into mathematical terms, our aim is to find a collection
	
	$$ 	\left\{ C_{i}\right\} _{i\in K}	$$
	
	such that
	$$ C_{i}\cap C_{j}=\emptyset,\;i\neq j $$
	
	and
	
	$$
	\bigcup_{i=1}^{k} C_{i} = C
	$$
	or, in other words, we want to partition $C$ into $k$ clusters.
	
	Moreover, for each cluster we wish to find a representative member to summarise the information contained within the cluster. An usual choice is to consider the mean, or \textbf{centroid}, of the cluster defined as
	\begin{equation*}
	\boldsymbol{c}_{i} = \frac{1}{n_{i}} \sum_{\boldsymbol{x}_{j} \in C_{i}} \boldsymbol{x}_{j},
	\end{equation*}
	where $n_{i}$ is the cardinality of $C_{i}$.\\
	
	The main issue is to find a method to partition the set $C$ into
	the clusters. A brute-force algorithm for finding a good clustering would imply generating all the possible partition of $n$ points into $k$ cluster and then, once an evaluation function has been set, take the best one. Informally, however, we can see that each point can be assigned to any of the $k$ clusters so that there are at most $k^{n}$ possible clusterings. Considering that any permutation of the $k$ clusters within a given clustering yields an equivalent result, we see that there are $O(\frac{k^{n}}{k!})$ clusterings of $n$ points into $k$ clusters. It is therefore clear that we need to rely on a different strategy.\\
	
	
	First of all, we need to define in a more precise manner
	the meaning of similarity between two points: we will consider the squared \textsl{Euclidean
		distance} for this purpose
	$$
	d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})= \sum_{k = 1}^{p} \left( x_{ik} - x_{jk}
	\right)^{2}.
	$$
	We now need to define some \textsl{loss} function and try to minimise
	it. The most natural idea follows from the assumption that members
	of one given cluster will be closer to each other than to member of
	other clusters: this is equivalent to stating that the correct partitioning
	will minimise the variability \textsl{within} the clusters. We need
	to translate this key concept into an objective function.
	
	We define
	$$
	I_{k}:=\left\{ i\in I\:s.t.\:\boldsymbol{x}_{i}\in C_{k}\right\} 
	$$
	
	and in this case we will have
	$$
	W(C_{k}) = \frac{1}{n_{k}} \underset{i,j\in I_{k}}{\sum} \sum_{l=1}^{p} \left(x_{il}-x_{jl}\right)^{2}
	$$
	and the problem becomes as follows
	\footnote{It is important to notice that this function surely decreases for
		increasing $k$ and in the extreme case of $k=n$ we have that each
		cluster contains only one point and we have no variability whatsoever.
		It is important to understand that we are looking for classes that
		do have some internal variability.}
	$$	\underset{C_{1},\ldots,C_{k}}{\min}\left\{ \sum_{k=1}^{K}W(C_{k})\right\} $$
	
	
	To solve the problem, K-Means applies a greedy iterative algorithm to find a local optimum.
	
	The centroids are randomly generated into the data space, usually considering a uniform distribution between the range for each dimension. Every iteration, then, consists of two main steps: $(1)$ cluster assignment and $(2)$ centroid update. That is to say, given the $k$ cluster means, in the cluster assignment step, each point $\boldsymbol{x_{j}}$ is assigned to the closest mean, which induces a clustering (each cluster $C_{i}$ comprises all the points that are closer to $\boldsymbol{c}_{i}$ than to any other centroid).
	\begin{equation*}
	i^{*}=\arg\underset{i\in K}{ \min}\left\{ d(\boldsymbol{x}_{j},\boldsymbol{c}_{i})\right\} =\arg\min_{i \in K}\left\{ d(\boldsymbol{x}_{j},\boldsymbol{c}_{1}),\ldots,d(\boldsymbol{x}_{j},\boldsymbol{c}_{k})\right\} \:\Longrightarrow\:\boldsymbol{x}_{j}\in C_{i^{*}}
	\end{equation*}
	Now, given a set of cluster $C_{i},\, i=1,...,k$, in the centroid update step new values are computed for each cluster from the points in $C_{i}$. The cluster assignment and centroid update steps are repeated until we reach a fixed point or a local minimum. In more practical words, we can say that K-Means converges if the centroids do not change from one iteration to the next and we can stop when the distance between an iteration and another is smaller than some positive threshold $\varepsilon$.
		
	The algorithm we have just defined is descent, that is it improves
	the solution at each iteration so that when the termination condition
	is true, we are certain to have reached a \textsl{local optimum}.
	Considering also that we randomly set the initial centroids, it is a good practice to run the algorithm several time and report the clustering with the evaluation value. It is also worth noting that K-Means generates convex-shaped clusters as the region in the data space corresponding to each cluster can be obtained intersecting the half-spaces resulting from the hyperplanes that bisect and are normal to the segments that join pairs of centroids.
		
	There is also one last important point to state. Any time we perform
	the K-means methods, we will find $k$ clusters on our data. Now,
	the question is: are we really separating actual subgroups or are
	we just \textsl{clustering the noise}? Moreover, suppose that we are
	dealing with a case in which the vast majority of data belongs to
	a small number of subgroups, but for a small subset that is quite
	different from the rest. As our method \textsl{forces} our data into
	a fixed number of clusters, the presence of an outlying class might
	bring to distorted results. So it is good practice not only to try
	various values of $k$ to see what happens, but also to try and apply
	to algorithm to subsets of our initial set to check that results remain
	somewhat stable.
	
	\subsubsection{Algorithm for K-means Clustering}
	\begin{enumerate}
		\item Randomly assign a number, from 1 to K, to each observation: they will
		serve as initial cluster assignments. Set $\mu:=0$
		\item Iterate what follows until termination criterion holds:
		\begin{enumerate}
			\item For each cluster, compute the \textsl{centroid} 
			$$
			\boldsymbol{c}_{i}^{\mu}=\frac{1}{card(C_{i}^{\mu})}\underset{j\in I_{i}^{\mu}}{\sum}\boldsymbol{x}_{i},
			$$
			computed as the mean vector of the observations in the $k$-th cluster
			at $\mu$-th iteration
			\item Assign each observation to the cluster whose centroid is closest (with
			respect to the Euclidean distance) as follows
			$$
			\forall j\in I,\;i^{*}=\underset{i\in K}{\min}\left\{ d(\underline{x}_{j},\underline{c}_{i}^{\mu})\right\} =\min\left\{ d(\underline{x}_{j},\underline{c}_{1}^{\mu}),\ldots,d(\underline{x}_{j},\underline{c}_{k}^{\mu})\right\} \:\Longrightarrow\:\underline{x}_{j}\in C_{i^{*}}^{\mu}
			$$
			\item Set $\mu:=\mu+1$
		\end{enumerate}
		\item Termination occurs when cluster assignments stop changing
	\end{enumerate}
	
	\subsection{DBSCAN}
	In our analysis of the DBSCAN algorithm, we shall be relying mainly on \cite{zaki2014dataminingbook}. The necessity of an algorithm adopting a different philosophy with respect to that of the K-Means algorithm arises from the fact that representative methods (such as the latter or, for instance, EM methods) are suitable for finding, at best, convex clusters.
	However, for non-convex clusters these methods are not capable to find the true clusters. This is because two points belonging to two different clusters might actually be close to each others than two points from the same on.
	For this reason, density-based clustering algorithms adopt a different paradigm, using the local densities of points to determine the clusters rather than relying solely on distances between couple of them.
	
	We now introduce some preliminary concepts. First of all, we define a \textbf{ball} of radius $\varepsilon$ around a point $\boldsymbol{x} \in \mathbb{R}$, called \textit{$\varepsilon$-neighbourhood} of $\boldsymbol{x}$, as follows:
	\begin{equation*}
	\mathcal{N}_{\varepsilon} (\boldsymbol{x}) = B_{\delta}\left( \boldsymbol{x}, \varepsilon \right) = \left\{ \boldsymbol{y} \; s.t. \; \delta \left( \boldsymbol{x}, \boldsymbol{y} \right) \leq \varepsilon \right\}.
	\end{equation*}
	Here $\delta \left( \boldsymbol{x}, \boldsymbol{y} \right)$ represents the distance between the points $\boldsymbol{x}$ and $\boldsymbol{y}$, which is usually assumed to be the Euclidean distance, that is
	\begin{equation*}
	\delta(\boldsymbol{x}, \boldsymbol{y}) = || \boldsymbol{x} - \boldsymbol{y} ||_{2}.
	\end{equation*}
	However, other distance metrics can be used.
	For any point $\boldsymbol{x} \in \boldsymbol{D}$, where $\boldsymbol{D}$ is the set of data points, we say that $\boldsymbol{x}$ is a \textit{core point} if there are at least $k$ points in its $\varepsilon$-neighbourhood.
	In other words, $\boldsymbol{x}$ is defined as a core point when it holds:
	\begin{equation*}
	| \mathcal{N}_{\varepsilon}(\boldsymbol{x})| \geq k,
	\end{equation*}
	where $k$ is a local density (or frequency) threshold defined by the user.
	
	A \textit{border point} is defined as a point that does not meet the $k$ threshold or, equivalently, that is such that $|\mathcal{N}_{\varepsilon}(\boldsymbol{x})| < k$, but belongs to the $\varepsilon$-neighbourhood of some core point $\boldsymbol{z}$.
	Finally, if a point is neither a core point nor a border point, it is called a \textit{noise point} or outlier.
	We say that a point $\boldsymbol{x}$ is \textit{directly density reachable} from another point $\boldsymbol{y}$ if $\boldsymbol{x} \in \mathcal{N}_{\varepsilon} \left( \boldsymbol{y} \right)$ and $\boldsymbol{y}$ is a core point.
	We say that $\boldsymbol{x}$ is \textit{density reachable} from $\boldsymbol{y}$ if there exists a chain of point, $\boldsymbol{x}_{0}, \boldsymbol{x}_{1}, ..., \boldsymbol{x}_{l}$, such that $\boldsymbol{x}=\boldsymbol{x}_{0}$ and $\boldsymbol{y} = \boldsymbol{x}_{l}$, and $\boldsymbol{x}_{i}$ is directly density reachable from $\boldsymbol{x}_{i-1}$ for all $i=1,...,l$.
	In other words, there must be a set of core points leading from $\boldsymbol{y}$ to $\boldsymbol{x}$. Note that the relationship of density reachability is an asymmetric property (or a directed relationship).
	Define any two points $\boldsymbol{x}$ and $\boldsymbol{y}$ to be density connected if there exists a core point $\boldsymbol{z}$, such that both $\boldsymbol{x}$ and $\boldsymbol{y}$ are density reachable from $\boldsymbol{z}$.
	A \textit{density based cluster} is defined as a maximal set of density connected points.
	
	The algorithm works as follows. First, DBSCAN computes the $\varepsilon$-neighbourhood $\mathcal{N}_{\varepsilon} \left( \boldsymbol{x}_{i} \right)$ for each point $\boldsymbol{x}_{i} \in \boldsymbol{D}$. It then checks if $\boldsymbol{x}_{i}$ is a core point. Moreover, it sets the cluster id $id\left(\boldsymbol{x}_{i}\right) = \emptyset$ for all points, indicating that they are not assigned to any cluster.
	Next, starting from each core point that is not assigned, the algorithm recursively finds all its density connected points, which are assigned to the same clusters. Some border point may be reachable from core points in more than on cluster: they may either be arbitrarily assigned to one of the clusters or to all of them (if overlapping clusters are allowed!).
	Those points that do not belong to any cluster are treated as outliers or noise.
	DBSCAN can be viewed from another point of view. In fact, we can see it as a search for connected components in a graph with the following characteristics: the vertices corresponds to the core points in the dataset and there exists an (undirected) edge between two vertices (core points) if the distance between them is less than $\varepsilon$, that is to say, if each of them is in the $\varepsilon$-neighbourhood of the other point.
	The connected components of this graph correspond to the core points of each cluster. Next, every core point incorporates into its cluster any border points in its neighbourhood.
	The biggest drawback of DBSCAN is given by its sensitivity to the value of $\varepsilon$, in particular if clusters have different densities. What happens is that is $\varepsilon$ is too small, than sparser clusters will be categorised as noise and if it is too large different clusters might end up being grouped together.
	\bibliographystyle{alpha}
	\bibliography{bibliography}	
\end{document}