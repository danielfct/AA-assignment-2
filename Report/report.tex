\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[british]{babel}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
%% To use hyperlinks
\usepackage{hyperref}
%% To use colors
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

%% To use maths
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
\usepackage{titlesec}

%% Command to type a chi character
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} 
%% Command to simulate a tab character
\newcommand\tab[1][1cm]{\hspace*{#1}}

%% Set sub of subsections
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}



\title{\textbf{Aprendizagem Autom\'{a}tica} \\
	\large Assignment 2 - Clustering}

\author{Andrea Mascaretti N52222\and Daniel Pimenta N45404}

\begin{document}
	\maketitle
	\begin{abstract}
		TODO
	\end{abstract}


	\section{Introduction}
	\section{Clustering Algorithms}
	\subsection{K-Means Algorithm}
	In our exposition of the K-Means Algorithm we rely heavily on \cite{hastie01statisticallearning}, \cite{MR2372475} and \cite{zaki2014dataminingbook}.
	Suppose that we have a set of $n$ observations, indexed by $I = \{1, ..., n\}$, of some quantitative
	variables
	$$ C = \{ \boldsymbol{x}_{1}, \boldsymbol{x}_{2}, ..., \boldsymbol{x}_{n} \} $$
	where
	
	$$ \boldsymbol{x}_{i} \in \mathbb{R}^{p},\;\forall i\in I$$
	
	
	We also have a set of labels (at this stage we will consider their
	number as given and such that $card(\mathcal{L})=k,\:k\in\left[1,n\right]\cap\mathbb{N}$)
	
	$$
	\mathcal{L}=\left\{ l_{1},\ldots,l_{k}\right\} ,\;K:=\left\{ 1,\ldots,k\right\} 
	$$
	Our goal is to construct a (\textit{measurable}) function to associate each
	and every element of $C$ to one label in $\mathcal{L}$. It is easy
	to see that it makes no difference to work with $\mathcal{L}$ or
	$K$ as we can always find a bijection between the two sets.
	
	We have reasons to believe that the elements belonging to one particular
	group are somewhat more similar to each other than they are to members
	of other groups and that one element can belong to one group and one
	group only. We also hypothesise that there are no empty groups. Translating
	this into mathematical terms, our aim is to find a collection
	
	$$ 	\left\{ C_{i}\right\} _{i\in K}	$$
	
	such that
	$$ C_{i}\cap C_{j}=\emptyset,\;i\neq j $$
	
	and
	
	$$
	\bigcup_{i=1}^{k} C_{i} = C
	$$
	or, in other words, we want to partition $C$ into $k$ clusters.
	
	Moreover, for each cluster we wish to find a representative member to summarise the information contained within the cluster. An usual choice is to consider the mean, or \textbf{centroid}, of the cluster defined as
	\begin{equation*}
	\boldsymbol{c}_{i} = \frac{1}{n_{i}} \sum_{\boldsymbol{x}_{j} \in C_{i}} \boldsymbol{x}_{j},
	\end{equation*}
	where $n_{i}$ is the cardinality of $C_{i}$.\\
	
	The main issue is to find a method to partition the set $C$ into
	the clusters. A brute-force algorithm for finding a good clustering would imply generating all the possible partition of $n$ points into $k$ cluster and then, once an evaluation function has been set, take the best one. Informally, however, we can see that each point can be assigned to any of the $k$ clusters so that there are at most $k^{n}$ possible clusterings. Considering that any permutation of the $k$ clusters within a given clustering yields an equivalent result, we see that there are $O(\frac{k^{n}}{k!})$ clusterings of $n$ points into $k$ clusters. It is therefore clear that we need to rely on a different strategy.\\
	
	
	First of all, we need to define in a more precise manner
	the meaning of similarity between two points: we will consider the squared \textsl{Euclidean
		distance} for this purpose
	$$
	d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})= \sum_{k = 1}^{p} \left( x_{ik} - x_{jk}
	\right)^{2}.
	$$
	We now need to define some \textsl{loss} function and try to minimise
	it. The most natural idea follows from the assumption that members
	of one given cluster will be closer to each other than to member of
	other clusters: this is equivalent to stating that the correct partitioning
	will minimise the variability \textsl{within} the clusters. We need
	to translate this key concept into an objective function.
	
	We define
	$$
	I_{k}:=\left\{ i\in I\:s.t.\:\boldsymbol{x}_{i}\in C_{k}\right\} 
	$$
	
	and in this case we will have
	$$
	W(C_{k}) = \frac{1}{n_{k}} \underset{i,j\in I_{k}}{\sum} \sum_{l=1}^{p} \left(x_{il}-x_{jl}\right)^{2}
	$$
	and the problem becomes as follows
	\footnote{It is important to notice that this function surely decreases for
		increasing $k$ and in the extreme case of $k=n$ we have that each
		cluster contains only one point and we have no variability whatsoever.
		It is important to understand that we are looking for classes that
		do have some internal variability.}
	$$	\underset{C_{1},\ldots,C_{k}}{\min}\left\{ \sum_{k=1}^{K}W(C_{k})\right\} $$
	
	
	To solve the problem, K-Means applies a greedy iterative algorithm to find a local optimum.
	
	The centroids are randomly generated into the data space, usually considering a uniform distribution between the range for each dimension. Every iteration, then, consists of two main steps: $(1)$ cluster assignment and $(2)$ centroid update. That is to say, given the $k$ cluster means, in the cluster assignment step, each point $\boldsymbol{x_{j}}$ is assigned to the closest mean, which induces a clustering (each cluster $C_{i}$ comprises all the points that are closer to $\boldsymbol{c}_{i}$ than to any other centroid).
	\begin{equation*}
	i^{*}=\arg\underset{i\in K}{ \min}\left\{ d(\boldsymbol{x}_{j},\boldsymbol{c}_{i})\right\} =\arg\min_{i \in K}\left\{ d(\boldsymbol{x}_{j},\boldsymbol{c}_{1}),\ldots,d(\boldsymbol{x}_{j},\boldsymbol{c}_{k})\right\} \:\Longrightarrow\:\boldsymbol{x}_{j}\in C_{i^{*}}
	\end{equation*}
	Now, given a set of cluster $C_{i},\, i=1,...,k$, in the centroid update step new values are computed for each cluster from the points in $C_{i}$. The cluster assignment and centroid update steps are repeated until we reach a fixed point or a local minimum. In more practical words, we can say that K-Means converges if the centroids do not change from one iteration to the next and we can stop when the distance between an iteration and another is smaller than some positive threshold $\varepsilon$.
		
	The algorithm we have just defined is descent, that is it improves
	the solution at each iteration so that when the termination condition
	is true, we are certain to have reached a \textsl{local optimum}.
	Considering also that we randomly set the initial centroids, it is a good practice to run the algorithm several time and report the clustering with the evaluation value. It is also worth noting that K-Means generates convex-shaped clusters as the region in the data space corresponding to each cluster can be obtained intersecting the half-spaces resulting from the hyperplanes that bisect and are normal to the segments that join pairs of centroids.
		
	There is also one last important point to state. Any time we perform
	the K-means methods, we will find $k$ clusters on our data. Now,
	the question is: are we really separating actual subgroups or are
	we just \textsl{clustering the noise}? Moreover, suppose that we are
	dealing with a case in which the vast majority of data belongs to
	a small number of subgroups, but for a small subset that is quite
	different from the rest. As our method \textsl{forces} our data into
	a fixed number of clusters, the presence of an outlying class might
	bring to distorted results. So it is good practice not only to try
	various values of $k$ to see what happens, but also to try and apply
	to algorithm to subsets of our initial set to check that results remain
	somewhat stable.
	
	\subsubsection{Algorithm for K-means Clustering}
	\begin{enumerate}
		\item Randomly assign a number, from 1 to K, to each observation: they will
		serve as initial cluster assignments. Set $\mu:=0$
		\item Iterate what follows until termination criterion holds:
		\begin{enumerate}
			\item For each cluster, compute the \textsl{centroid} 
			$$
			\boldsymbol{c}_{i}^{\mu}=\frac{1}{card(C_{i}^{\mu})}\underset{j\in I_{i}^{\mu}}{\sum}\boldsymbol{x}_{i},
			$$
			computed as the mean vector of the observations in the $k$-th cluster
			at $\mu$-th iteration
			\item Assign each observation to the cluster whose centroid is closest (with
			respect to the Euclidean distance) as follows
			$$
			\forall j\in I,\;i^{*}=\underset{i\in K}{\min}\left\{ d(\underline{x}_{j},\underline{c}_{i}^{\mu})\right\} =\min\left\{ d(\underline{x}_{j},\underline{c}_{1}^{\mu}),\ldots,d(\underline{x}_{j},\underline{c}_{k}^{\mu})\right\} \:\Longrightarrow\:\underline{x}_{j}\in C_{i^{*}}^{\mu}
			$$
			\item Set $\mu:=\mu+1$
		\end{enumerate}
		\item Termination occurs when cluster assignments stop changing
	\end{enumerate}
	
	\subsection{DBSCAN}
	
	\bibliographystyle{alpha}
	\bibliography{bibliography}	
\end{document}